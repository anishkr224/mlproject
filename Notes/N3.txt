Lec: 268

1. EDA STUDENT PERFORMANCE.ipynb
2. MODEL TRAINING.ipynb

Download: ipykernel is required for implementing or executing any code in the Jupyter notebook.

In model training, what all techniques we have specifically used; we are basically going to convert the EDA into a modular coding.

During development, avoid using -e . in requirements.txt to prevent rebuilding the package repeatedly. Instead, comment it out and only uncomment it at the end when you're ready to build the final package.

pip install -r requirements.txt

git add .
git status
git commit -m "EDA and Model Training"
git push -u origin main

Lec: 269

data_ingestion.py
It's important to consistently write logs throughout the project, as they help track where exceptions occur and make it easier to debug issues by identifying the exact line causing the error.

(D:\mlproject\venv) D:\mlproject>python -m src.components.data_ingestion

Go to .gitignore file; in # Environments add .artifacts/

git add .
git status
git commit -m "Data Ingestion"
git push -u origin main

Lec: 270

data_transformation.py

Data ingestion: we read dataset from a source, performed a train-test split, and saved the data inside an artifact folder.

Data transformation, involves feature engineering and data cleaning, such as converting categorical features into numerical features.

ColumnTransformer is used to build a pipeline where you can apply multiple transformations — like one-hot encoding for categorical features and standard scaling for numerical features — in a structured and efficient way.

SimpleImputer is a tool used for handling missing values in a dataset. Many machine learning algorithms don’t work well with missing data, so imputing (filling in) missing values is an essential preprocessing step.
Common strategies: mean, median, most_frequent, constant.

In utils.py, define a utility function save_object to save any Python object as a pickle file. It ensures the directory exists and uses dill to dump the object.

src\pipeline\train_pipeline.py file to check data_transformation.py is properly working or not.

(D:\mlproject\venv) D:\mlproject>python -m src.pipeline.train_pipeline

git add .
git status
git commit -m "Data Transformation"
git push -u origin main

Lec: 271

model_trainer.py

In utils.py, define a utility function evaluate_models to trains multiple models, evaluates them on train and test datasets using R2 score, and returns a dictionary of test scores for each model.

Update the train_pipeline.py file to check model_trainer.py is properly working or not.

python -m src.pipeline.train_pipeline

Go to .gitignore file; in # Environments remove .artifacts/

git add .
git status
git commit -m "Model Trainer"
git push -u origin main

Lec: 272 - Model Hyperparameter Tuning Implementation

In model_trainer.py: Implemented hyperparameter tuning by integrating parameter grids for multiple machine learning algorithms.
In evaluate_models function: Utilized GridSearchCV from scikit-learn to perform cross-validated parameter search.

python -m src.pipeline.train_pipeline

git add .
git status
git commit -m "Hyperparameter tuning done"
git push -u origin main

Lec: 273 - Building Prediction Pipeline

src\pipeline\predict_pipeline.py : To capture user input features and make predictions on input features.
In utils.py, define a utility function load_object to loads a Python object from the given file path using dill.

app.py : Flask based web application.
templates\index.html
templates\home.html

python app.py

http://127.0.0.1:5000/predictdata

git add .
git status
git commit -m "Prediction pipeline done"
git push -u origin main